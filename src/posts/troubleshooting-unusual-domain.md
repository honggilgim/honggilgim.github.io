## 운영 사용하지 않은 url으로 요청이 들어올 경우

웹사이트 또는 홈페이지를 운영하다 보면, 사용하지 않는 도메인으로 요청이 들어오는 경우가 있다. 이런 경우는 보통 코드 혹은 데이터에서 이 url로 접근하는 방식, 사용자가 직접 url을 쳐서 들어가는 경우 등 많은 경우의 수가 있을 수 있지만, 명확하지 않은 추측으로 어떻게 접근했는지 모르겠다는 말은 개발자가 해서는 안되는 말이다.

최근에 사용하지 않는 url 혹은 특이한 url로 접근하는 경우가 많이 발생했고 이에 대한 케이스 정리를 해두려 한다.

## 1. FCKeditor 관련 취약점 스캔 공격

**실제 인입 url**: `#{홈페이지 주소}/FCKeditor/editor/filemanager/connectors/asp/connector.asp`

우리 사이트는 해당 url을 사용하지 않고, FCKeditor라는 툴도 사용하지 않는다. 처음에 보고 어디서 들어왔지? 하며 벙찐 경험이 있는 url이다.

이 FCKeditor은 오래된 FCKeditor의 파일 업로드 취약점으로 유명한 url이다. 이 취약점에 그대로 노출될 경우, 서버에 실제로 파일이 업로드된다. 이 경우에는 큰 문제가 되지만 우리 사이트는 FCKeditor를 안 쓴다.

**들어온 원인**: FCKeditor는 해커의 광범위한 웹 스캔 공격으로 여러 사이트에 해당 요청을 보내 취약점을 확인하곤 한다. 이 원인으로 광범위한 해커의 웹 스캔 공격에 요청이 발생했다.

## 2. PHP 스캔 공격

**실제 인입 url**: `#{홈페이지 주소}/menu.php`, `#{홈페이지 주소}/users.php`

실제 운영중인 사이트는 PHP를 사용하지 않는 사이트였다. 이 공격은 워드프레스를 사용하는지 체크하기 위해 유입된 url로 판별났고, 실제 워드프레스 보안 취약점이 발표된 당일에 해당 인입이 확인되었다. 워드프레스를 사용하는 사이트라면 관련 취약점을 업데이트 하는 등의 조치를 취해야 하지만, 우리 사이트는 워드프레스가 아니여서 따로 조치를 취하지 않았다.

## 3. robots.txt, sitemap.xml 로 인한 유입

**실제 인입 url**: 이전에 운용하다 변경된 url

이전에 운용하다 기능 자체를 변경할 경우 기존의 url은 남겨두는 경우가 많다. 이런 경우에 발생한 오류인데, 사용하지 않는 url에 갑작스럽게 요청이 날아와 이와 관련된 조사 중 알게 된 내용이다.

robots.txt, sitemap.xml 파일은 검색 엔진에 가이드를 주는 문서이다. 이 문서들은 보통 자동화 툴을 사용해서 만들곤 하지만, 우리 사이트는 전부 수동으로 제작되었다. url이 들어온 경로의 톰캣 로그를 살펴보니, 이 파일들을 읽고 난 후 접근해 검색 크롤러가 접근했다고 판단했다.

